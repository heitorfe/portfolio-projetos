<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Project Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link id="theme-link" rel="stylesheet" href="assets/css/dark.css">
		<link rel="stylesheet" href="assets/css/carousel.css">

	</head>
	
	
	<body class="is-preload">
		

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Project Portfolio</strong> in data and software</a>
									<ul class="icons">
									</ul>
								</header>

								<!-- languages -->
								<header id="lang" style="display: flex; align-items: center; gap: 10px;">
									<a href="https://heitorfe.github.io/portfolio-projetos/index" class="image"><img src="images/bra.svg" width=50 height=40 alt=""></a>
									<a href="#" class="image"><img src="images/usa.svg" width=50 height=40 alt=""></a>
									<button id="theme-toggle">Toggle Theme</button>
								</header>



								</header>


							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h1>Hello, welcome to my project portfolio!</h1>
										</header>
										<p>On this page you will learn a little about my journey and my skills in <strong>Data Science, Analysis and Engineering</strong>, <strong>Artificial Intelligence</strong> and <strong>Cloud Infrastructure</strong>, through projects with public data.</p>
									</div>
									<span class="image object">
										<img src="images/pic10.jpg" alt="" />
									</span>
								</section>


								<!-- About me -->
									<section>
										<header class="major">
											<h1>About me</h1>
										</header>
											<h2 id="content">My name is Heitor Felix</h2>
											<p>I hold a degree in Data Science from Uninter and currently work as a <strong>Data Engineer</strong> at NTT Data, building and optimizing data pipelines in <strong>Databricks</strong> for the financial sector. I have over 3 years of experience in Data Engineering, with strong expertise in Data Lakehouse architectures, AI solutions development (RAG/LLMs), and cloud infrastructure implementation. I hold Databricks certifications (Associate and Professional) and Azure certifications (DP-203, AZ-900, DP-900). Explore my projects below and feel free to reach out.</p>

									</section>

<!-- Section -->
<section>
	<header class="major">
		<h1>Skills</h1>
	</header>
	<div class="features">
		<article>
			<span class="icon solid fa-code"></span>
			<div class="content">
				<h3>Data Engineering and Science:</h3>
				<ul>
					<li>Python for data engineering and science</li>
					<li>Advanced feature creation with pandas</li>
					<li>Databases: SQL Server, PostgreSQL, Google Big Query and Snowflake</li>
					<li>Data pipelines with Azure Data Factory and Databricks</li>
					<li>dbt Core</li>
					<li>Data pipelines with Azure Data Factory and Databricks</li>
					<li>Orchestration with Apache Airflow</li>
					<li>Elasticsearch</li>
				</ul>
			</div>
		</article>
		<article>
			<span class="icon solid fa-project-diagram"></span>
			<div class="content">
				<h3>Machine Learning and AI:</h3>
				<ul>
					<li>Regression, classification and clustering models</li>
					<li>Creation of customized LLMs with RAG</li>
					<li>Automated deployment in different environments (DevOps and DataOps)</li>
					<li>Fundamentals of statistics and mathematics</li>
				</ul>
			</div>
		</article>
		<article>
			<span class="icon solid fa-chart-line"></span>
			<div class="content">
				<h3>Data Visualization and Communication:</h3>
				<ul>
					<li>Power BI, Tableau, Metabase</li>
					<li>Python: Matplotlib, Seaborn, Plotly</li>
					<li>Advanced Excel</li>
					<li>Streamlit dashboards</li>
				</ul>
			</div>
		</article>
		<article>
			<span class="icon solid fa-cloud"></span>
			<div class="content">
				<h3>Infrastructure and Cloud Computing:</h3>
				<ul>
					<li>Solid experience with Azure (certifications AZ-900, DP-900, DP-203)</li>
					<li>Experience with AWS</li>
					<li>Infrastructure as Code (IaC) with Bicep template</li>
					<li>API creation in Python and serverless deployment</li>
					<li>CI/CD in different environments with Azure DevOps and GitHub Actions</li>
				</ul>
			</div>
		</article>
	</div>
</section>

<!-- Certifications (loaded from shared/certifications.html) -->
<div id="certifications-placeholder"></div>

								<!-- Experience -->
								<section>
									<header class="major">
										<h1>Professional Experience</h1>
									</header>
									<div class="project">
										<h2>Data Engineer at NTT Data</h2>
										<span class="experience-period">2025 - Present</span>
										<p>Building and optimizing data pipelines in <strong>Databricks</strong> for financial sector clients. Working on migration projects to modern <strong>Data Lakehouse</strong> architectures, ensuring performance, scalability, and data governance.</p>
									</div>
									<div class="project">
										<h2>Data Engineer II at Sapiensia Tecnologia</h2>
										<span class="experience-period">2022 - 2024</span>
										<p>Technical leadership in data engineering projects, developing critical pipelines on <strong>Azure</strong> and <strong>Databricks</strong>. Implemented AI solutions with <strong>RAG and LLMs</strong> for process automation. Architected <strong>disaster recovery</strong> strategies and serverless infrastructure. Responsible for analytical dashboards and Python automations that directly impacted business decisions.</p>
									</div>
									<div class="project">
										<h2>Data Science Intern at 027capital</h2>
										<span class="experience-period">2021 - 2022</span>
										<p>Development of <strong>churn prediction</strong> models and data ingestion pipelines using <strong>Python</strong> and <strong>Google Cloud</strong>. Created software for financial data processing and analysis.</p>
									</div>
								</section>

							<!-- Section -->
							<section>
								<header class="major">
									<h1>Projects</h1>
								</header>
								<div class="posts">
									<article class="featured en">
										<a href="https://github.com/heitorfe/olist-lakehouse-2.0" class="image"><img src="images/olist2.png" alt="Olist Lakehouse 2.0 Architecture" /></a>
										<h3>Advanced Data Lakehouse: Olist Lakehouse 2.0</h3>
										<div class="tech-tags">
											<span class="tech-tag data-eng">Databricks</span>
											<span class="tech-tag data-eng">Delta Lake</span>
											<span class="tech-tag data-eng">dbt</span>
											<span class="tech-tag cloud">Azure</span>
											<span class="tech-tag data-eng">PySpark</span>
										</div>
										<p>This is the second version of the Olist project, now implementing modern data engineering practices in the <strong>Databricks Lakehouse</strong> with focus on declarative pipelines, governance, and incremental processing at scale.
											The architecture follows the <strong>Medallion pattern (Bronze, Silver, Gold)</strong> using <strong>Lakeflow Declarative Pipelines</strong> (formerly Delta Live Tables), with continuous ingestion via <strong>AutoLoader</strong> and change tracking with <strong>AUTO CDC</strong> for SCD Type 1 and Type 2 dimensions.
											The project incorporates <strong>Data Quality with expectations</strong> at all layers, centralized governance with <strong>Unity Catalog</strong>, data lineage, and separation of pipelines for append-only transactional data and entities with historical changes.
											All infrastructure and pipelines are deployed via <strong>Databricks Asset Bundles</strong>, with automated CI/CD, code validation, testing, and deployment across development, staging, and production environments.</p>
										<div class="row">
											<div class="col-6 col-12-small">
												<h4>Tools used</h4>
												<ul>
													<li>Databricks Lakehouse & Delta Lake</li>
													<li>Lakeflow Declarative Pipelines (DLT)</li>
													<li>AutoLoader (incremental ingestion)</li>
													<li>AUTO CDC (SCD Type 1 and Type 2)</li>
													<li>Unity Catalog (governance and lineage)</li>
													<li>Databricks Asset Bundles (IaC)</li>
													<li>GitHub Actions (CI/CD)</li>
													<li>SQL, PySpark</li>
												</ul>
												<ul class="actions">
													<li><a href="https://github.com/heitorfe/olist-lakehouse-2.0" class="button">Learn more</a></li>
												</ul>
											</div>
										</div>
									</article>

									<article>
										<a href="https://github.com/heitorfe/pipeline-deputados" class="image"><img src="images/deputados.png" alt="Deputies project diagram" /></a>
										<h3>Brazilian Congress Deputies Data Pipeline</h3>
										<p>Complete data pipeline focused on data engineering, with automated ingestion of public information from all federal deputies. 
											The data includes biography, mandates, expenses and parliamentary activity, extracted via official API. 
											The architecture implements a modern ELT approach with <strong>Snowflake and dbt</strong> . 
											Daily incremental ingestion is orchestrated with <strong>Airflow</strong> and stored in S3 in Parquet format. 
											Transformations follow robust dimensional modeling patterns (SCD Type 2). 
											The project ensures end-to-end scalability, automation and data quality.<p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Python, Pandas and requests</li>
														<li>Apache Airflow</li>
														<li>Amazon S3 and SQS</li>
														<li>Snowflake, Snowpipe</li>
														<li>dbt Core</li>
														<li>Streamlit and Jupyter notebook</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfe/pipeline-deputados" class="button">Learn more</a></li>
														<li><a href="https://medium.com/@heitorfelix/building-a-modern-data-warehouse-a-practical-guide-to-dbt-data-modeling-with-real-world-examples-1a0071b15c72" class="button">Read article</a></li>
													</ul>
												</div>
											</div>
										</article>
									<article>
										<a href="#" class="image"><img src="images/olist.png" alt="Olist project diagram" /></a>
										<h3>Data Lakehouse: Olist</h3>
										<p>This project used the <strong>Databricks Data Lakehouse architecture</strong> to manage data in layers (Raw, Bronze, Silver, and Gold) and simulate ingestion scenarios with CDC (Change Data Capture). The data, from a Kaggle dataset, was enriched to create a complete pipeline, from ingestion to business analysis.

											I implemented data governance with Unity Catalog, orchestration with Databricks Workflows, and continuous integration via GitHub Actions. The project consolidated skills in data pipelines, automation, and analysis with the Medallion architecture, optimizing the use of data for insights and analytical applications. <p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Pandas</li>
														<li>Git, GitHub, GitHub Actions</li>
														<li>Azure Blob Storage, Parquet</li>
														<li>Databricks, UnityCatalog</li>
														<li>Spark, Delta Lake</li>
														<li>Databricks Workflows</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfelix/olist-pipeline" class="button">Learn more</a></li>
													</ul>
												</div>
											</div>
									</article>


									<article>
										<a href="#" class="image"><img src="images/rag.png" alt="Diagram of the OCR project" /></a>
										<h3>Chatbot with GPT-4 and Azure</h3>
										<p>In this project, I explored Azure Artificial Intelligence tools to build a chatbot specialized in Azure using GPT-4. I copied the data from the Azure documentation on GitHub to the Storage Account, used Azure AI Search to perform embedding and indexing of the content, and Azure OpenAI to build the chatbot in an App on Azure. The goal is to provide accurate and contextualized answers about Azure services and functionalities.<p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Python</li>
														<li>Azure Blob Storage</li>
														<li>Azure AI Search</li>
														<li>Azure OpenAI</li>
														<li>Git, GitHub</li>
														<li>Bicep template (IaC)</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfe/RAG-llm-chatbot" class="button">Learn more</a></li>
													</ul>
												</div>
											</div>
									</article>

									<article>
										<a href="#" class="image"><img src="images/ocr.png" alt="Diagram of the OCR project" /></a>
										<h3><strong>IN PROGRESS:</strong> Telegram Bot: Text Recognition (Computer Vision)</h3>
										<p>In this project, I explored Azure Artificial Intelligence tools for optical character recognition (OCR), such as <strong>Azure Computer Vision</strong> and <strong>Azure AI Document</strong> Intelligence. I used Python to develop a Telegram bot that processes images sent by the user, returning the extracted text and the confidence interval for each recognized word. I implemented dynamic settings in the bot, allowing adjustment of parameters such as the minimum confidence level to accept words and the application of pre-processing. This project demonstrates skills in API integration, image processing and the creation of interactive interfaces with bots.<p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Python</li>
														<li>Telegram API</li>
														<li>Azure Computer Vision</li>
														<li>Azure AI Document Intelligence</li>
														<li>Git, GitHub</li>
														<li>Bicep template (IaC)</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfelix/OCR-telegram-bot" class="button">Learn more</a></li>
													</ul>
												</div>
											</div>
									</article>



								</div>
							</section>

							<!-- Section -->
							<section>
								<header class="major">
									<h1>Older Projects (2021 - 2022)</h1>
								</header>
								<div class="posts">
									<article>
										<a href="#" class="image"><img src="images/datachallenge.png" alt="" /></a>
										<h3>Stone Data Challenge 2022</h3>
										<p>I was a semifinalist in the Stone Data Challenge 2022. In this Stone challenge, my task was to use historical data from a loan program from 2019 to April 2022 from 14,700 clients. The business problem was related to contacting clients who were behind on payments. The question to be answered was: What is the ideal curve of times we should contact a client? To answer it, I used Python and Power BI to answer the question with data analysis. <p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Git, GitHub, LSF Git files</li>
														<li>Python, Pandas, Seaborn, Plotly</li>
														<li>Power BI</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfe/data_challenge_stone/" class="button">Learn more</a></li>
														<li><a href="https://app.powerbi.com/view?r=eyJrIjoiYTA4MmMyYTYtNTA2NC00ODAxLTljZjctYzUwYWE3ZmE2MDgwIiwidCI6Ijg3MDk0MzhmLTMzNDItNGI0Yy1hZDY5LTNkMjFlMmY4OTZlOSJ9" class="button">Dashboard</a></li>
													</ul>
												</div>
											</div>
									</article>

									<article>
										<a href="#" class="image"><img src="images/sales.jpg" alt="" /></a>
										<h3>Sales Prediction</h3>
										<p>I used Python to create a Machine Learning model to predict the sales of each of the 3,000 registered stores in the next 6 weeks. The model was put into production and can be requested via API by Telegram, just needing internet access to use it. The model had a 90% prediction of the real value, allowing the CFO to make decisions based on the future revenue of each store unit and thus be able to make investments without losses.</p>
										<!-- Lists -->
										<div class="row">
											<div class="col-6 col-12-small">
												<h4>Tools used</h4>
												<ul>
													<li>Git, GitHub</li>
													<li>Python, Pandas, Seaborn, Boruta</li>
													<li>Scikit-Learn and Scipy</li>
													<li>Flask</li>
													<li>Heroku Cloud</li>
													<li>Telegram API</li>
												</ul>
												<ul class="actions">
													<li><a href="https://github.com/heitorfe/rossman-sales-predict" class="button">Learn more</a></li>
												</ul>
											</div>
										</div>
									</article>

									<article>
										<a href="#" class="image"><img src="images/cross-sell.jpg" alt="" /></a>
										<h3>Classification of customers most likely to buy</h3>
										<p>I used Python to create a Machine Learning model to rank the customers most likely to purchase a new product (cross-sell strategy). With an accuracy of 33.5% for the top 20,000 customers in the database, the sales team is able to reach interested parties with much less cost.<p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Git, GitHub</li>
														<li>Python, Pandas, Seaborn, Extra Tree Classifier</li>
														<li>Scikit-Learn, Scipy and Scikit-Plot</li>
														<li>Flask</li>
														<li>Heroku Cloud</li>
														<li>Google Sheets API with Google Scripts</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfe/health_insurance_cross_sale" class="button">Learn more</a></li>
													</ul>
												</div>
											</div>
									</article>

									<article>
										<a href="#" class="image"><img src="images/cluster.jpg" alt="" /></a>
										<h3>Customer loyalty with clustering</h3>
										<p>I used Python to create a Machine Learning model to find the "Insiders," the best customers of the company. The objective of this project was to group customers with similar behaviors so that the business team can build personalized actions, based on the characteristics of each cluster. <p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Git, GitHub</li>
														<li>Python, Pandas, Seaborn, GMM</li>
														<li>Scikit-Learn, Scipy and Yellowbrick</li>
														<li>SQLite</li>
														<li>Metabase</li>
														<li>Papermill</li>
													</ul>
													<ul class="actions"></ul>
														<li><a href="https://github.com/heitorfe/insiders_clustering" class="button">Learn more</a></li>
													</ul>
												</div>
											</div>
									</article>

									<article>
										<a href="#" class="image"><img src="images/designkc.png" alt="" /></a>
										<h3>House Rocket Data Analysis</h3>
										<p>I used Python and Power BI to perform exploratory data analysis and thus confirmed or not some hypotheses about the business, resulting in insights for better business performance. The analysis aimed to increase the revenue of the fictitious company, House Rocket, which works with the buying and selling of real estate, finding the best times to buy or sell the property. <p>

											<!-- Lists -->
											<div class="row">
												<div class="col-6 col-12-small">
													<h4>Tools used</h4>
													<ul>
														<li>Git, GitHub</li>
														<li>Python, Pandas, Seaborn, Plotly</li>
														<li>Geopy API</li>
														<li>Power BI</li>
														<li>SQLite</li>
													</ul>
													<ul class="actions">
														<li><a href="https://github.com/heitorfe/kc-house-insights" class="button">Learn more</a></li>
														<li><a href="https://app.powerbi.com/view?r=eyJrIjoiNzExYzgyMDQtZjU5ZC00YzAzLTk4MzItYTI3MGQ2ZDM5MDJhIiwidCI6Ijg3MDk0MzhmLTMzNDItNGI0Yy1hZDY5LTNkMjFlMmY4OTZlOSJ9&pageName=ReportSection25f6ac0f4d2653587e84" class="button">Dashboard</a></li>
													</ul>
												</div>
											</div>
									</article>
								</div>
							</section>

								

								<!-- Contact (loaded from shared/contact.html) -->
							<div id="contact-placeholder"></div>

						</div>
					</div>


			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/theme-toggle.js"></script>
			<script src="assets/js/carousel.js"></script>
			<script src="assets/js/shared-loader.js"></script>

	</body>
</html>